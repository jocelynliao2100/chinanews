# streamlit_app.py

import streamlit as st
from docx import Document
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import re
from datetime import datetime
import io
import pandas as pd
import os

# å˜—è©¦è¼‰å…¥ jieba
import jieba
import jieba.analyse

# å˜—è©¦è¼‰å…¥ BeautifulSoupï¼Œè‹¥ä¸å­˜åœ¨å°±è¨­ç‚º None
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None
# å˜—è©¦è¼‰å…¥ tqdmï¼Œè‹¥ä¸å­˜åœ¨å°±ç”¨ identity function
try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x: x

# --- Font Setup ---
from matplotlib.font_manager import FontProperties

LOCAL_FONT_PATH = "NotoSansCJKtc-Regular.otf"  # æ”¾åœ¨åŒç›®éŒ„
font_installed = False
font_prop = None
if os.path.exists(LOCAL_FONT_PATH):
    try:
        font_prop = FontProperties(fname=LOCAL_FONT_PATH)
        plt.rcParams["font.family"] = font_prop.get_name()
        plt.rcParams["axes.unicode_minus"] = False
        font_installed = True
    except Exception as e:
        st.warning(f"è¼‰å…¥å­—é«”å¤±æ•—ï¼Œå°‡ä½¿ç”¨é è¨­å­—é«”ï¼š{e}")
        plt.rcParams["axes.unicode_minus"] = False
else:
    st.info(f"æ‰¾ä¸åˆ° {LOCAL_FONT_PATH}ï¼Œå¦‚éœ€ä¸­æ–‡è«‹è‡ªè¡Œéƒ¨ç½²å­—é«”ã€‚")
    plt.rcParams["axes.unicode_minus"] = False

st.set_page_config(page_title="åœ‹å°è¾¦æ–°èç¨¿åˆ†æ", layout="wide")
st.title("ğŸ‡¨ğŸ‡³ åœ‹å°è¾¦æ–°èç¨¿åˆ†æå·¥å…·")

# ========== Helper Functions ==========

@st.cache_data(show_spinner=False)
def process_uploaded_files(uploaded_files, col_names):
    """
    è®€å…¥å¤šå€‹ .docxï¼Œè§£ææ®µè½ä¸­çš„æ—¥æœŸ (2020-01 ~ 2025-04)ï¼Œ
    å›å‚³ {col_name: { "YYYY-MM": count }} çµæ§‹ã€‚
    """
    date_re = re.compile(
        r"\b(202[0-5])[-/å¹´\.](0?[1-9]|1[0-2])[-/æœˆ\.](0?[1-9]|[12]\d|3[01])æ—¥?\b"
    )
    counts = defaultdict(lambda: defaultdict(int))

    for name, uploaded in zip(col_names, uploaded_files):
        data = uploaded.getvalue()
        doc = Document(io.BytesIO(data))
        for para in doc.paragraphs:
            m = date_re.search(para.text)
            if not m:
                continue
            y, mth, _ = m.groups()
            ym = f"{y}-{int(mth):02d}"
            dt = datetime(int(y), int(mth), 1)
            if datetime(2020, 1, 1) <= dt <= datetime(2025, 4, 30):
                counts[name][ym] += 1
    return counts

def plot_counts(counts, col_names, display_names):
    """
    ç¹ªè£½æŠ˜ç·šåœ–ï¼Œä¸¦åœ¨ Streamlit ä¸­é¡¯ç¤ºã€‚
    """
    # å…ˆæ”¶é›†æ‰€æœ‰æœˆä»½
    all_months = sorted({ym for d in counts.values() for ym in d})
    # è½‰æˆ datetime listï¼Œä¸¦æ ¼å¼åŒ– labels
    dates = [datetime.strptime(ym, "%Y-%m") for ym in all_months]

    fig, ax = plt.subplots(figsize=(12, 6))
    for orig, disp in zip(col_names, display_names):
        yvals = [counts[orig].get(ym, 0) for ym in all_months]
        if font_installed:
            ax.plot(dates, yvals, marker="o", label=disp, fontproperties=font_prop)
        else:
            ax.plot(dates, yvals, marker="o", label=disp)

    title = "2020.01â€“2025.04 åœ‹å°è¾¦å„æ¬„ç›®æ–°èç¨¿æ•¸é‡è®ŠåŒ–"
    if font_installed:
        ax.set_title(title, fontproperties=font_prop)
        ax.set_xlabel("å¹´æœˆ", fontproperties=font_prop)
        ax.set_ylabel("æ•¸é‡", fontproperties=font_prop)
    else:
        ax.set_title(title)
        ax.set_xlabel("å¹´æœˆ")
        ax.set_ylabel("æ•¸é‡")

    ax.legend()
    ax.grid(True)
    plt.xticks(rotation=45)
    st.pyplot(fig)

def parse_list_docx(file_bytes):
    """
    å¾ Word è§£æå‡º [YYYY-MM-DD] æ¨™è¨˜çš„æ¨™é¡Œèˆ‡ URL åˆ—è¡¨ã€‚
    å›å‚³ list of dict(date, title, url)ã€‚
    """
    try:
        doc = Document(io.BytesIO(file_bytes))
        text = "\n".join(p.text for p in doc.paragraphs)
        items = []
        for line in text.splitlines():
            dm = re.search(r"\[(202\d-\d{1,2}-\d{1,2})\]", line)
            if not dm:
                continue
            date_str = dm.group(1)
            tm = re.search(r"\]\s*(.*?)\s*(http", line)
            um = re.search(r"(https?://\S+)", line)
            title = tm.group(1).strip() if tm else ""
            url = um.group(1).strip() if um else ""
            try:
                dt = datetime.strptime(date_str, "%Y-%m-%d")
                if datetime(2020,1,1) <= dt <= datetime(2025,4,30):
                    items.append({"date": dt, "title": title, "url": url})
            except:
                continue
        return items
    except Exception as e:
        st.error(f"è§£æåˆ—è¡¨å¤±æ•—ï¼š{e}")
        return []

def fetch_content(url):
    """
    ç°¡å–®ç”¨ requests + BeautifulSoup æŠ“å– class="TRS_Editor" çš„æ–‡å­—ã€‚
    """
    if BeautifulSoup is None:
        return ""
    try:
        r = requests.get(url, timeout=5)
        r.encoding = r.apparent_encoding
        soup = BeautifulSoup(r.text, "html.parser")
        editor = soup.find("div", class_="TRS_Editor")
        return editor.get_text("\n").strip() if editor else ""
    except Exception as e:
        st.warning(f"æŠ“å– {url} å¤±æ•—ï¼š{e}")
        return ""

# ========== ä¸»ç¨‹å¼ ==========

# 1ï¸âƒ£ æ–°èç¨¿æ•¸é‡è®ŠåŒ–
st.header("1. æ–°èç¨¿æ•¸é‡è®ŠåŒ–åˆ†æ")

uploaded_counts = st.file_uploader(
    "ä¸€æ¬¡ä¸Šå‚³äº”å€‹ .docx æª”æ¡ˆ (å°è¾¦å‹•æ…‹ã€äº¤æµäº¤å¾€ã€æ”¿å‹™è¦èã€éƒ¨é–€æ¶‰å°ã€æ–°èç™¼ä½ˆ)",
    type="docx", accept_multiple_files=True, key="uploader_counts"
)

display_names = ["å°è¾¦å‹•æ…‹", "äº¤æµäº¤å¾€", "æ”¿å‹™è¦è", "éƒ¨é–€æ¶‰å°", "æ–°èç™¼ä½ˆ"]

if uploaded_counts:
    if len(uploaded_counts) != 5:
        st.warning(f"è«‹ä¸Šå‚³äº”å€‹æª”ï¼Œç›®å‰ {len(uploaded_counts)} å€‹")
    else:
        filenames = [f.name for f in uploaded_counts]
        counts = process_uploaded_files(uploaded_counts, filenames)
        plot_counts(counts, filenames, display_names)

        # é¡å¤–æŒ‡æ¨™ï¼šæœ€å¤šæœˆ & è©²æœˆé—œéµå­—
        # æ‰¾å‡ºå“ªä¸€å€‹ YYYY-MM ç¸½å’Œæœ€å¤§
        total = Counter()
        for d in counts.values():
            total.update(d)
        most_month, most_cnt = total.most_common(1)[0]
        st.metric("ğŸ¯ æ–°èç¨¿æœ€å¤šæœˆä»½", most_month, f"{most_cnt} ç¯‡")

        # æŠ“å‡ºè©²æœˆæ‰€æœ‰æ®µè½ï¼Œåšå‰20é—œéµè©
        segments = []
        # é‡æ–°è®€æ®µè½ä»¥ä¿ç•™æ–‡å­—
        date_re = re.compile(
            r"\b(202[0-5])[-/å¹´\.](0?[1-9]|1[0-2])[-/æœˆ\.](0?[1-9]|[12]\d|3[01])æ—¥?\b"
        )
        for up in uploaded_counts:
            doc = Document(io.BytesIO(up.getvalue()))
            for p in doc.paragraphs:
                m = date_re.search(p.text)
                if m:
                    ym = f"{m.group(1)}-{int(m.group(2)):02d}"
                    if ym == most_month:
                        segments.append(p.text)

        words = []
        for seg in segments:
            words += [w for w in jieba.lcut(seg) if len(w) > 1]
        top20 = Counter(words).most_common(20)

        st.subheader(f"ğŸ“‘ {most_month} å‰20é—œéµè©")
        cols = st.columns(2)
        for i, (w, cnt) in enumerate(top20):
            cols[i % 2].write(f"{i+1}. {w}ï¼š{cnt}")

# 2ï¸âƒ£ å–®ç¯‡é—œéµè©åˆ†æ
st.markdown("---")
st.header("2. å–®ç¯‡æ–°èç¨¿é—œéµè©åˆ†æ")

uploaded_kw = st.file_uploader(
    "ä¸Šå‚³å–®ä¸€ .docx é€²è¡Œé—œéµè©åˆ†æ", type="docx", key="uploader_kw"
)
if uploaded_kw:
    data = uploaded_kw.getvalue()
    doc = Document(io.BytesIO(data))
    txt = "\n".join(p.text for p in doc.paragraphs)
    tags = jieba.analyse.extract_tags(txt, topK=30, withWeight=True)
    st.subheader("å‰30é—œéµè© (å«æ¬Šé‡)")
    for w, wt in tags:
        st.write(f"{w}ï¼š{wt:.3f}")

# 3ï¸âƒ£ åˆ—è¡¨è§£æèˆ‡å…¨æ–‡çˆ¬å–
st.markdown("---")
st.header("3. æ–°èåˆ—è¡¨è§£æèˆ‡å…¨æ–‡çˆ¬å–")

uploaded_list = st.file_uploader(
    "ä¸Šå‚³å«åˆ—è¡¨çš„ .docx (æ ¼å¼ï¼š[YYYY-MM-DD] æ¨™é¡Œ http...)", 
    type="docx", key="uploader_list"
)
if uploaded_list:
    data = uploaded_list.getvalue()
    items = parse_list_docx(data)
    if items:
        st.success(f"å…±è§£æåˆ° {len(items)} å‰‡æ–°è")
        # é¡¯ç¤ºæ¨™é¡Œèˆ‡é€£çµ
        for it in items:
            st.write(f"- {it['date'].date()}  [{it['title']}]({it['url']})")
        # çˆ¬å–å…¨æ–‡
        contents = []
        with st.spinner("çˆ¬å–ä¸­..."):
            for it in tqdm(items):
                contents.append(fetch_content(it["url"]))
        st.success(f"æˆåŠŸçˆ¬å– {sum(bool(c) for c in contents)} ç¯‡")
        full_text = "\n\n".join(contents)
        st.subheader("å…¨æ–‡æ‘˜è¦å‰ 500 å­—")
        st.write(full_text[:500] + "...")
    else:
        st.warning("æœªèƒ½è§£æä»»ä½•åˆ—è¡¨é …ç›®ï¼Œè«‹ç¢ºèªæ ¼å¼ã€‚")
