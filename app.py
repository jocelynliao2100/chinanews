import streamlit as st
from docx import Document
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import matplotlib as mpl
import re, io, os
from datetime import datetime
import pandas as pd
import jieba
import jieba.analyse
import requests

# å˜—è©¦è¼‰å…¥ BeautifulSoupï¼Œä¸å½±éŸ¿å­—é«”
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

# --- 1. Font Setup (Matplotlib) ---
LOCAL_FONT_PATH = "NotoSansCJKjp-Regular.otf"
if os.path.exists(LOCAL_FONT_PATH):
    # 1.1 åŠ å…¥åˆ° Matplotlib çš„ fontManager
    mpl.font_manager.fontManager.addfont(LOCAL_FONT_PATH)
    # 1.2 è¨­å®šç‚ºå…¨åŸŸå­—é«”
    mpl.rcParams["font.family"] = mpl.font_manager.FontProperties(fname=LOCAL_FONT_PATH).get_name()
    mpl.rcParams["axes.unicode_minus"] = False
else:
    st.warning(f"æ‰¾ä¸åˆ°å­—é«”æª” {LOCAL_FONT_PATH}ï¼Œä¸­æ–‡å¯èƒ½é¡¯ç¤ºä¸æ­£å¸¸ã€‚")

# --- 2. å·¥å…·å‡½å¼ ---
def process_uploaded_files(uploaded_files, column_names):
    date_re = re.compile(
        r"\b(202[0-5])[-/å¹´\.](0?[1-9]|1[0-2])[-/æœˆ\.](0?[1-9]|[12]\d|3[01])æ—¥?\b"
    )
    counts = defaultdict(lambda: defaultdict(int))

    for name, uploaded in zip(column_names, uploaded_files):
        data = uploaded.getvalue()
        doc = Document(io.BytesIO(data))
        for p in doc.paragraphs:
            m = date_re.search(p.text)
            if not m:
                continue
            y, mth, _ = m.groups()
            ym = f"{y}-{int(mth):02d}"
            dt = datetime(int(y), int(mth), 1)
            if datetime(2020,1,1) <= dt <= datetime(2025,4,30):
                counts[name][ym] += 1

    # è½‰æˆä¸€èˆ¬ dict
    return {k: dict(v) for k, v in counts.items()}

def plot_data(counts, original_names, display_names):
    # å…¨éƒ¨æœˆä»½
    all_m = sorted({ym for d in counts.values() for ym in d})
    # è½‰ datetime
    dates = [datetime.strptime(ym, "%Y-%m") for ym in all_m]

    fig, ax = plt.subplots(figsize=(10,5))
    for orig, disp in zip(original_names, display_names):
        y = [counts.get(orig,{}).get(ym, 0) for ym in all_m]
        ax.plot(dates, y, marker="o", label=disp)

    ax.set_title("2020.01â€“2025.04 åœ‹å°è¾¦å„æ¬„ç›®æ–°èç¨¿æ•¸é‡è®ŠåŒ–")
    ax.set_xlabel("å¹´æœˆ")
    ax.set_ylabel("æ–°èç¨¿æ•¸é‡")
    ax.legend()
    ax.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    st.pyplot(fig)

# --- 3. Streamlit App ---

st.title("ğŸ‡¨ğŸ‡³ åœ‹å°è¾¦æ–°èç¨¿åˆ†æ")

st.header("ä¸€ã€æ–°èç¨¿æ•¸é‡è®ŠåŒ–")
uploaded = st.file_uploader(
    "è«‹ä¸€æ¬¡ä¸Šå‚³ 5 å€‹ .docxï¼ˆå°è¾¦å‹•æ…‹ã€äº¤æµäº¤å¾€ã€æ”¿å‹™è¦èã€éƒ¨é–€æ¶‰å°ã€æ–°èç™¼ä½ˆï¼‰",
    type="docx", accept_multiple_files=True, key="uploader1"
)
display_names = ["å°è¾¦å‹•æ…‹", "äº¤æµäº¤å¾€", "æ”¿å‹™è¦è", "éƒ¨é–€æ¶‰å°", "æ–°èç™¼ä½ˆ"]

if uploaded:
    if len(uploaded) != 5:
        st.warning(f"è«‹ä¸Šå‚³ 5 å€‹æª”ï¼Œç›®å‰ï¼š{len(uploaded)}")
    else:
        originals = [f.name for f in uploaded]
        with st.spinner("ğŸ”„ è³‡æ–™è™•ç†ä¸­..."):
            cnts = process_uploaded_files(uploaded, originals)
        plot_data(cnts, originals, display_names)

        # é¡å¤–ï¼šæ‰¾å‡ºæœ€å¤šæœˆä»½ & è©²æœˆå‰20é—œéµå­—
        total = Counter()
        for d in cnts.values():
            total.update(d)
        if total:
            month, num = total.most_common(1)[0]
            st.metric("ğŸ¯ æ–°èç¨¿æœ€å¤šæœˆä»½", month, f"{num} ç¯‡")

            # è’é›†è©²æœˆæ‰€æœ‰æ®µè½
            segs = []
            date_re = re.compile(
                r"\b(202[0-5])[-/å¹´\.](0?[1-9]|1[0-2])[-/æœˆ\.](0?[1-9]|[12]\d|3[01])æ—¥?\b"
            )
            for up in uploaded:
                doc = Document(io.BytesIO(up.getvalue()))
                for p in doc.paragraphs:
                    m = date_re.search(p.text)
                    if m and f"{m.group(1)}-{int(m.group(2)):02d}"==month:
                        segs.append(p.text)

            words = [w for seg in segs for w in jieba.lcut(seg) if len(w)>1]
            top20 = Counter(words).most_common(20)
            st.subheader(f"ğŸ“‘ {month} å‰20é—œéµè©")
            cols = st.columns(2)
            for i, (w,c) in enumerate(top20):
                cols[i%2].write(f"{i+1}. {w}ï¼š{c}")

st.markdown("---")
st.header("äºŒã€å–®ç¯‡é—œéµè©åˆ†æ")
single = st.file_uploader("ä¸Šå‚³å–®ç¯‡æ–°èç¨¿ (.docx)", type="docx", key="uploader2")
if single:
    data = single.getvalue()
    doc = Document(io.BytesIO(data))
    txt = "\n".join(p.text for p in doc.paragraphs)
    tags = jieba.analyse.extract_tags(txt, topK=30, withWeight=True)
    st.subheader("å‰30é—œéµè©(å«æ¬Šé‡)")
    for w,wt in tags:
        st.write(f"{w}ï¼š{wt:.3f}")

st.markdown("---")
st.header("ä¸‰ã€æ–°èåˆ—è¡¨è§£æèˆ‡å…¨æ–‡çˆ¬å–")
list_up = st.file_uploader("ä¸Šå‚³å«åˆ—è¡¨çš„ Word æª”", type="docx", key="uploader3")
def parse_list_docx(buf):
    items=[]
    doc=Document(io.BytesIO(buf))
    for line in "\n".join(p.text for p in doc.paragraphs).splitlines():
        m=re.search(r"\[(\d{4}-\d{1,2}-\d{1,2})\]\s*(.*?)\s*(https?://\S+)?", line)
        if m:
            d,t,u=m.groups()
            try:
                dt=datetime.strptime(d, "%Y-%m-%d")
                if datetime(2020,1,1)<=dt<=datetime(2025,4,30):
                    items.append({"date":dt, "title":t or "", "url":u or ""})
            except: pass
    return items

def fetch_text(url):
    if not BeautifulSoup: return ""
    try:
        r=requests.get(url,timeout=5); r.encoding=r.apparent_encoding
        s=BeautifulSoup(r.text,"html.parser").find("div",class_="TRS_Editor")
        return s.get_text("\n") if s else ""
    except: return ""

if list_up:
    buf=list_up.getvalue()
    lst=parse_list_docx(buf)
    if lst:
        st.success(f"è§£æåˆ° {len(lst)} ç¯‡æ–°è")
        texts=[]
        with st.spinner("çˆ¬å–å…¨æ–‡ä¸­..."):
            for it in lst:
                if it["url"].startswith("http"):
                    texts.append(fetch_text(it["url"]))
        st.success(f"æˆåŠŸçˆ¬å– {len([t for t in texts if t])} ç¯‡")
    else:
        st.warning("ç„¡æ³•è§£æä»»ä½•æ–°èåˆ—è¡¨")
